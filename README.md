# Study-09-MachineLearning-C-[Deep Learning]
DeepLearning Intro

----------------------------------------------------------------------------------------------------------------------------------------
## What if the data is not linearly separable ?
<img src="https://user-images.githubusercontent.com/31917400/41435452-c25ebfae-7016-11e8-808c-8e740d608806.jpg" />

**How to Combine two linear models into a non-linear model?**: Each linear model is a whole probability space, which means for every points, it gives us **the probability of the point being 'positive'**. And we have two linear models, thus we get two probability values.  
 - Example01: We add up two probabilities, then pass into the Sigmoid function, which gives us the final probability value!
 - Example02: But what if we need to weight this sum? 
   - We take a **[LINEAR-COMBINATION]** of the two linear models(thinking of the **new line** of between the two models)
<img src="https://user-images.githubusercontent.com/31917400/41471198-de1cd444-70aa-11e8-9908-cabaf9373110.jpg" />

### Deep Neural Network(More complex network and multiple layers):
   - Basic 3 Layers
     - Input-layer: input values(from field_A, field_B,..or from the Sigmoid_A, Sigmoid_B..) that constitute each datapoint.  
     - Hidden-layer: a set of linear models generated by the Input-layer, and probabilities from the Sigmoid(). 
     - Output-layer: where the two linear models(two Sigmoid-outputs) get combined to obtain a non-linear model.
   - If adding more **nodes** to the input, hidden, and output layers?
     - What happens if the Hidden-layer has more nodes?
       - We combine more linear models and obtain a triangular boundary in the Output-layer. 
     - What happens if the Input-layer has more nodes?
       - our linear models become planes..and our output-layer bounds a nonlinear region.
     - What if the Output-layer has more nodes?
       - We get more outputs. This is when we have a multiclass classification model. We'll have each node in the Output-layer outputting a score for each one of the classes(one for A, one for B...)
   - If adding more **layers**?
     - Our linear models combine to create non-linare models, then these combine to create even more non-linear models.
     - In this way, the network will split the n-dimensional space with a highly non-linear boundary as the output.  
<img src="https://user-images.githubusercontent.com/31917400/41473766-1f9e9f9a-70b2-11e8-8839-e1e67a215c09.jpg" />
     
 - Multiclass Classification: If our neural network needs to model data with more than one output?
   - Simply add more nodes in the Output-layer!
   - We take the scores and apply the **softmax()** function to obtain well-defined probabilities.  
<img src="https://user-images.githubusercontent.com/31917400/41533886-49e55b1a-72f4-11e8-8313-12ec22f57952.jpg" />
<img src="https://user-images.githubusercontent.com/31917400/41535449-446184f6-72fa-11e8-9b38-95b8439c72fa.jpg" />

### How do neural networks process the input to obtain an output? 
### 1.Feedforward
What parameters(W,b) should they have on the edges(x1, x2) in order to model our data well? 
 - The perceptron(the simplist NN) here is defined by a linear model where W1 > W2.
 - Then the perceptron plots the points(x1, x2) and outputs the odds that the point is positive.
<img src="https://user-images.githubusercontent.com/31917400/41541346-1e378cac-730a-11e8-90e5-fe82148dbaf9.jpg" />
     
 - Error-Function(How badly each point is being classified? How far from the line?)
<img src="https://user-images.githubusercontent.com/31917400/41541969-af95ec10-730b-11e8-9eda-194d5e58301a.jpg" />

### 2.Backpropagation
 - 1. Doing a feedforward operation
 - 2. Comparing the **output** of the model with the **desired output**.
 - 3. Calculating the error.
 - 4. Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.
 - 5. Use this to update the weights, and get a better model.
 - 6. Continue this until we have a model that is good.

[GradientDescentAlgorithm & Backpropagation]
> Single Perceptron
 - We calculate the `Gradient` of the Error-Function E(W)
   - What the misclassified-point want: the **boundary** to come closer to it then, the boundary get closer to it by updating (W,b).
   - We continue doing this to minimize the error. 
> Multi-layer Perceptron
 - The Error-Function is more complicated, thus we do Backpropagation:
   - What the misclassified-point want: the **(+) region** to come closer to it then, 
     - when looking at the two linear models in the hidden-layer, we can see which one is doing better.
     - so we care the better linear model more than the other.
       - Reduce the `W` coming from the loser and increase the `W` coming from the winner.
     - or we go back to the hidden layer and for the loser model, we have its boundary get closer to the point by updating (W,b), and for the winner model, we have its boundery move farther away from the point by updating (W,b).    
<img src="https://user-images.githubusercontent.com/31917400/41565629-73289c8e-734f-11e8-8a7c-28ab563b0141.jpg" />




































